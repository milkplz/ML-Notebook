{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 필요한 라이브러리\n",
    "numpy tensorflow keras skimage\n",
    "```\n",
    "$ pip3 install numpy tensorflow keras skimage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import keras\n",
    "from numpy import array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "DEEPFASHION_DATA_DIR = os.path.join(ROOT_DIR, \"fashion_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepfashion dataset을 load하고,  필요한 데이타를 메모리에 올립니다.\n",
    "Deepfashion dataset에서 category, imagepath, bbox 데이타를 load한다.\n",
    "txt파일들은 모두 첫번째 라인은 list 개수가 적혀 있고, 두번째 라인은 데이타 항목이 적혀 있다.\n",
    "numpy에서 matrix로 load하기 위해 첫번째 라인과 두번째라인 삭제한다. (수동으로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " ...\n",
      " [41.]\n",
      " [41.]\n",
      " [41.]]\n",
      "[[ 72.  79. 232. 273.]\n",
      " [ 67.  59. 155. 161.]\n",
      " [ 65.  65. 156. 200.]\n",
      " ...\n",
      " [ 18.  41. 149. 230.]\n",
      " [ 75.  47. 220. 300.]\n",
      " [ 36.  63. 202. 296.]]\n",
      "[['img/Sheer_Pleated-Front_Blouse/img_00000001.jpg']\n",
      " ['img/Sheer_Pleated-Front_Blouse/img_00000002.jpg']\n",
      " ['img/Sheer_Pleated-Front_Blouse/img_00000003.jpg']\n",
      " ...\n",
      " ['img/Paisley_Print_Babydoll_Dress/img_00000052.jpg______________________4']\n",
      " ['img/Paisley_Print_Babydoll_Dress/img_00000053.jpg______________________4']\n",
      " ['img/Paisley_Print_Babydoll_Dress/img_00000054.jpg______________________4']]\n",
      "[[     0]\n",
      " [     1]\n",
      " [     2]\n",
      " ...\n",
      " [289219]\n",
      " [289220]\n",
      " [289221]]\n",
      "(289222, 1) (289222, 1) (289222, 1) (289222, 4)\n"
     ]
    }
   ],
   "source": [
    "# category\n",
    "val_category = np.genfromtxt(os.path.join(DEEPFASHION_DATA_DIR, \"Anno\", \"list_category_img.txt\"))\n",
    "# string과 number가 섞여 있기 때문에 첫번째 인자가 non으로 오므로 삭제한다.\n",
    "val_category = np.delete(val_category, 0, 1)\n",
    "print(val_category)\n",
    "\n",
    "# bbox\n",
    "val_bbox = np.genfromtxt(os.path.join(DEEPFASHION_DATA_DIR, \"Anno\", \"list_bbox.txt\"))\n",
    "# string과 number가 섞여 있기 때문에 첫번째 인자가 non으로 오므로 삭제한다.\n",
    "val_bbox = np.delete(val_bbox, 0, 1)\n",
    "print(val_bbox)\n",
    "\n",
    "# image path\n",
    "tmp_imagepath = []\n",
    "with open(os.path.join(DEEPFASHION_DATA_DIR, \"Anno\", \"list_category_img.txt\")) as list_category_img:\n",
    "    for row in list_category_img:\n",
    "        imagepath = row.strip()[:-1].strip().replace(' ', '_')\n",
    "        tmp_imagepath.append(imagepath)\n",
    "        \n",
    "val_imagepath = array(tmp_imagepath)\n",
    "val_imagepath = val_imagepath.reshape((-1, 1))\n",
    "print(val_imagepath)\n",
    "\n",
    "\n",
    "# image path index\n",
    "val_imagepath_idx = np.arange(val_imagepath.size)\n",
    "val_imagepath_idx = val_imagepath_idx.reshape((-1, 1))\n",
    "print(val_imagepath_idx)\n",
    "\n",
    "\n",
    "print(val_imagepath.shape, val_imagepath_idx.shape, val_category.shape, val_bbox.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset for ML\n",
    "1.Load한 data들의 Array를 하나의 Array로 합친다.\n",
    "2.shuffle한다.\n",
    "3.80%는 train set으로 20%는 test set으로 분류한다.\n",
    "    x_train, y_train, x_test, y_test 으로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.concatenate((val_imagepath_idx, val_bbox, val_category), axis = 1) \n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# 80%는 train set으로 20%는 test set으로 분류\n",
    "dataset_threshold = 231377;\n",
    "train_dataset = dataset[:dataset_threshold,]\n",
    "test_dataset = dataset[dataset_threshold:,]\n",
    "\n",
    "train_x = train_dataset[:,:5]\n",
    "train_y = train_dataset[:,5:]\n",
    "\n",
    "test_x = test_dataset[:,:5]\n",
    "test_y = test_dataset[:,5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
